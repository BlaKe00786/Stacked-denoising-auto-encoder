# -*- coding: utf-8 -*-
"""
Created on Sat Apr 13 14:37:37 2019

@author: BlaKe
"""

import os

import torch
from torch import nn
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import MNIST
from torchvision.utils import save_image

if not os.path.exists('./mlp_img'):
    os.mkdir('./mlp_img')
    
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = torch.device("cpu")

def to_img(x):
    x = x.view(x.size(0), 1, 28, 28)
    return x

num_epochs = 15
batch_size = 100
learning_rate = 0.1

def add_noise(img):
    noise = torch.randn(img.size()) * 0.2
    noisy_img = img + noise
    return noisy_img

def plot_sample_img(img, name):
    img = img.view(1, 28, 28)
    save_image(img, './sample_{}.png'.format(name))
    
def min_max_normalization(tensor, min_value, max_value):
    min_tensor = tensor.min()
    tensor = (tensor - min_tensor)
    max_tensor = tensor.max()
    tensor = tensor / max_tensor
    tensor = tensor * (max_value - min_value) + min_value
    return tensor

# replaces all elements in-place with the values of the elements of tensor rounded to the nearest integers
def tensor_round(tensor):
    return torch.round(tensor)

img_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),
    transforms.Lambda(lambda tensor:tensor_round(tensor))
])

dataset = MNIST('./data', transform=img_transform, download=True)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)


class L1Penality(torch.autograd.Function):
  
  """
  In the forward pass we receive a Tensor containing the input and return
  a Tensor containing the output. ctx is a context object that can be used
  to stash information for backward computation. You can cache arbitrary
  objects for use in the backward pass using the ctx.save_for_backward method.
  """
  @staticmethod
  def forward(ctx, input, l1weight):
    ctx.save_for_backward(input)
    ctx.l1weight = l1weight
    return input
  
  """
  In the backward pass we receive a Tensor containing the gradient of the loss
  with respect to the output, and we need to compute the gradient of the loss
  with respect to the input.
  """
  @staticmethod
  def backward(ctx, grad_output):
    input, = ctx.saved_tensors
    grad_input = input.clone().sign().mul(ctx.l1weight)
    grad_input += grad_output
    return grad_input, None


class autoencoder(nn.Module):
    def __init__(self, input_size, output_size, stride):
        super(autoencoder, self).__init__()

        self.encoder = nn.Sequential(
            nn.Conv2d(input_size, output_size, kernel_size=2, stride=stride, padding=0),
            nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(output_size, input_size, kernel_size=2, stride=2, padding=0), 
            nn.ReLU(),
        )


    def forward(self, x):
        x = self.encoder(x)
        x = L1Penality.apply(x, 0.1)
        x = self.decoder(x)
        return x
    

class StackedAutoEncoder(nn.Module):
    r"""
    A stacked autoencoder made from the convolutional denoising autoencoders above.
    Each autoencoder is trained independently and at the same time.
    """

    def __init__(self):
        super(StackedAutoEncoder, self).__init__()

        self.ae1 = autoencoder(784, 100, 1)
        self.ae2 = autoencoder(100, 256, 1)
        self.ae3 = autoencoder(256, 512, 1)

    def forward(self, x):
        a1 = self.ae1(x)
        a2 = self.ae2(a1)
        a3 = self.ae3(a2)

        if self.training:
            return a3

        else:
            return a3, self.reconstruct(a3)

    def reconstruct(self, x):
            a2_reconstruct = self.ae3.reconstruct(x)
            a1_reconstruct = self.ae2.reconstruct(a2_reconstruct)
            x_reconstruct = self.ae1.reconstruct(a1_reconstruct)
            return x_reconstruct


#model = autoencoder().cuda()
model = StackedAutoEncoder()
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(
    model.parameters(), lr=learning_rate)



for epoch in range(num_epochs):
    for data in dataloader:
        img, _ = data
        img = img.view(img.size(0), -1)
        noisy_img = add_noise(img)
        #noisy_img = Variable(noisy_img).cuda()
        #img = Variable(img).cuda()
        noisy_img = Variable(noisy_img)
        img = Variable(img)
        # ===================forward=====================
        output = model(noisy_img)
        loss = criterion(output, img)
        MSE_loss = nn.MSELoss()(output, img)
        # ===================backward====================
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    # ===================log========================
    print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'
          .format(epoch + 1, num_epochs, loss.data, MSE_loss.data))
    if epoch % 10 == 0:
        x = to_img(img.cpu().data)
        x_hat = to_img(output.cpu().data)
        x_noisy = to_img(noisy_img.cpu().data)
        weights = to_img(model.encoder[0].weight.cpu().data)
        save_image(x, './mlp_img/x_{}.png'.format(epoch))
        save_image(x_hat, './mlp_img/x_hat_{}.png'.format(epoch))
        save_image(x_noisy, './mlp_img/x_noisy_{}.png'.format(epoch))
        save_image(weights, './filters/epoch_{}.png'.format(epoch))

